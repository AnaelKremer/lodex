append = pack
label = query-openalex
extension = json
mimeType = application/json

# load some plugins to activate some statements
[use]
plugin = conditor
plugin = basics
plugin = analytics

# Toggle ezs traces (see server stderr log)
[debug]
ezs = true

# {{{
[TXTConcat]

[env]
path = url
value = https://api.openalex.org/works

path = query
value = self().trim()

[replace]
path = filter
value = env('query')

path = cursor
value = *

[URLRequest]
timeout = 60000
url = env('url')

[loop]
test = has('meta.next_cursor')

[loop/replace]
path = filter
value = env('query')

path = cursor
value = get('meta.next_cursor')

[loop/URLRequest]
url = env('url')
timeout = 60000

[loop/drop]
path = meta.next_cursor

[exploding]
value = results

[exchange]
value = get('value')

[assign]
path = uri
value = get('id').replace('https://openalex.org/', '')

path = apil_doi
value = get('doi').replace("https://doi.org/","").toLower()

path = apil_is_oa
value = get("open_access.is_oa").replace(false,"Non").replace(true,"Oui")

path = apil_keywords
value = get('keywords').map('display_name')

# On teste si la liste des urls où sont déposés les "works" au moins une appartient à un portail Hal.
# La regex couvre l'ensemble des portails hal et la spécificité de leurs urls.
[assign]
path = apil_is_hal
value = get("locations").map("landing_page_url").compact().some(item=>(/[^a-zA-Z0-9]hal[^a-zA-Z0-9]/).test(item)).replace(false,"Non").replace(true,"Oui")

# On récupère le champ qui indique dans quelles bases les "works" sont indexés.
# On concatène le champ "apil_is_hal", on retire les false et transforme les true en "hal"
[assign]
path = apil_indexed_in
value = get("indexed_in").concat(self.apil_is_hal).pull("Non").map(item=>item === "Oui" ? "hal" : item).sort()

[assign]
path = author_name
value = get('authorships').flatMap("raw_author_name")

[assign]
path = institutions
value = get('authorships').flatMap("raw_affiliation_strings")

# On récupère l'éditeur de la revue. ([] si vide, nécéssaire pour les instructions et fonctions futures).
[assign]
path = apil_publisher_only
value = get("primary_location.source.host_organization_name","n/a")

# On récupère un tableau avec l'éditeur mais aussi, s'il en a, sa ou ses sociétés mères (3 niveaux de hiérarchie).
# L'ordre est aléatoire, on ne peut se baser sur l'index. (["n/a"] si vide, nécéssaire pour les instructions et fonctions futures).
[assign]
path = apil_linked_publishers
value = get("primary_location.source.host_organization_lineage_names",["n/a"])

# On compare les 2 champ créés au-dessus, pour retirer du tableau les éditeurs du plus-bas niveau de hiérarchie.
# Si publisherOnly était au plus haut niveau (0) on obtient [].
# S'il était au niveau 1 on obtient son niveau supérieur (0).
# S'il était au plus bas on obtient ses 2 sociétés mères (0 & 1).
[assign]
path = apil_xor_publisher
value = get("apil_publisher_only").castArray().xor(self.apil_linked_publishers)

# On isole tous les tableaux vides.
[swing]
test = get("apil_xor_publisher").isEmpty()

# On leur réattribue leur unique valeur.
[swing/assign]
path = apil_xor_publisher
value = get("apil_publisher_only")

# On associe le tout, ne reste plus que le cas des tableaux avec niveaux 0 et 1.
# On retire les parents directs des éditeurs de niveau 2, on n'a donc plus que les éditeurs du plus haut niveau.
[assign]
path = apil_publisher
value = get("apil_xor_publisher").pull("Springer Science+Business Media","Vandenhoeck & Ruprecht","University of California, Berkeley","University of California, Los Angeles","Siberian Branch of the Russian Academy of Sciences").toString().replace(null,"n/a").replace(undefined,"n/a")

path = source
value = get("primary_location.source.display_name")

path = apil_oa_status
value = get("apc_list.value").replace(/^(?!0$).*$/,"").replace(/^0$/,"diamond").concat(self.open_access.oa_status).compact().reduce((acc, v) => {if (v === "diamond") {return ["diamond"]} {return acc}},)

path = is_retracted
value = get("is_retracted").replace(false,"Non").replace(true,"Oui")

path = apil_pages
value = get("biblio.first_page").concat(self.biblio.last_page).uniq().join(" to ")

path = apil_volume
value = get("biblio.volume")

path = apil_issue
value = get("biblio.issue")

path = apil_funder_display_name
value = get("grants").map("funder_display_name").uniq()

[assign]
path = apil_funders_award_id
value = get("grants").map(obj=>[obj.funder_display_name,obj.award_id]).map(arr=>_.compact(arr)).map(item => item.join(" : "))

[swing]
test = get("apil_funders_award_id").isEmpty()

[swing/assign]
path = apil_funders_award_id
value = fix(["n/a"])

## On retire les notices non pertinentes en raison d'erreurs d'affiliations d'OpenAlex.
# On récupère dans un premier temps les adresses originales qu'OpenALex a identifié comme pertinentes.
#[assign]
#path = apil_raw_affiliations_filtered
#value = get("authorships").filter(obj => obj.institutions.some(obj => obj.display_name === "I4210133362")).flatMap(obj => obj.raw_affiliation_strings)

# Dans un second temps on fait passer quelques regex pour vérifier que les adresses originales matchent bien avec le laboratoire visé.
#[assign]
#path = apil_verif_raw_affiliations_filtered
#value = get("apil_raw_affiliations_filtered").some(item => /ganil/i.test(item) || /grand acc.*national.*ions.*lourds/i.test(item) || /Large heavy ion Nat.*acc.*/i.test(item))

# Enfin on supprime les notices non pertinentes.
#[remove]
#test = get("apil_verif_raw_affiliations_filtered").isEqual(false)

# On recupère l'abstract sous forme d'un tableau d'objets où keys sont les mots et values leur(s) position(s) dans l'abstract.
# Les keys contiennent donc des "." ou autres caractères spéciaux proscrits par MongoDB Node.js driver.
# On transforme cela en un tableau où chaque value est "key:value", on découpe pour avoir une matrice. On parse les values en base 10 pour obtenir des entiers.
# On inverse ensuite les éléments dans chaque sous-tableau de la matrice. On effectue un tri dans la matrice, on ne garde plus que les mots et transforme le tout en un string.
[assign]
path = apil_abstract
value = get("abstract_inverted_index").flatMap((values, key) => {return values.map(value => `${key}:${value}`) }).map(item => item.split(":")).map(([first, second]) => [first, parseInt(second, 10)]).map(item => item.reverse()).sort((a, b) => a[0] - b[0]).map(item => item.slice(1)).flatMap().join(' ')

[exchange]
value = omit(["doi","biblio","abstract_inverted_index","indexed_in","display_name","apil_publisher_only","apil_xor_publisher","publication_date","countries_distinct_count","institutions_distinct_count","corresponding_author_ids","corresponding_institution_ids","has_fulltext","fulltext_origin","cited_by_percentile_year","is_paratext","primary_topic","keywords","locations_count","referenced_works","related_works","ngrams_url","cited_by_api_url","created_date","updated_date","grants"])

# }}}

# Ensures that each object contains an identification key (required by lodex)
[swing]
test = pick(['URI', 'uri']).pickBy(_.identity).isEmpty()
[swing/identify]

# Ignore objects with duplicate URI
[dedupe]
ignore = true

# Prevent keys from containing dot path notation (which is forbidden by nodejs mongoDB driver)
[OBJFlatten]
separator = fix('.')
reverse = true
safe = true

# Uncomment to see each data sent to the database
#[debug]

# Add contextual metadata related to the import
[assign]
path = lodexStamp.importedDate
value = fix(new Date()).thru(d => d.toDateString())
path = lodexStamp.usedParser
value = env('parser')
path = lodexStamp.query
value = env('query')
path = uri
value = get('uri').trim()

